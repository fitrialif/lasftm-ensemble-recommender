{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LastFM Recommender\n",
    "The aim of this exercies is to build an ensemble recommender for music artists, using data made available [here](http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-360K.tar.gz).\n",
    "\n",
    "I'll follow the methods described in [Jeremy Howards video](https://www.youtube.com/watch?v=V2h3IOBDvrA&t=5761s).\n",
    "\n",
    "This data set is a single table with 350k users organised into the following rows:\n",
    "- UserID \n",
    "- ArtistID \n",
    "- ArtistName \n",
    "- PlayCount\n",
    "\n",
    "Not all of the artists have a valid ArtistID, so we will use the artist name as a unique id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN not available)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'data/lastfm-dataset-360K/'\n",
    "fulldata_file = 'usersha1-artmbid-artname-plays.tsv'\n",
    "data_file = 'fulldata.tsv'\n",
    "sample_file = 'sampledata.tsv'\n",
    "\n",
    "def read_fulldata_file():\n",
    "    return pd.read_csv(path + fulldata_file, \n",
    "                       sep='\\t',\n",
    "                       usecols=[0,2,3],\n",
    "                       names=['user', 'artist','plays'])\n",
    "\n",
    "def read_sample_file():\n",
    "    return pd.read_csv(path + sample_file,\n",
    "                       sep='\\t')\n",
    "\n",
    "# Create the sample dataset of 1000 users\n",
    "if not os.path.isfile(path + sample_file):\n",
    "    df = read_fulldata_file()\n",
    "    users_to_sample = df.user.sample(n=5000)\n",
    "    rows_to_sample = df[df.user.isin(users_to_sample)]\n",
    "    rows_to_sample.to_csv(path + sample_file,\n",
    "                          index=False,\n",
    "                          sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = read_sample_file()\n",
    "# df = read_fulldata_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to transform the data somewhat:\n",
    "- UserID and Artist name to continguous integers\n",
    "- Playcount value for each (user, artist) tuple into a normalized value representing how much the user likes that artist compared to other artists that they have listened to.\n",
    "\n",
    "We shall assign each (user, artist) tuple a value representing the fraction of all of that users plays that the artist represents. This should then leave the value normalized between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userid2ids = {o:i for i,o in enumerate(df.user.unique())}\n",
    "artistid2ids = {o:i for i,o in enumerate(df.artist.unique())}\n",
    "total_plays_per_user = df.groupby(['user'])['plays'].sum().to_dict()\n",
    "\n",
    "def normalize(row):\n",
    "    row['plays'] = row['plays'] / total_plays_per_user[row['user']]\n",
    "    row['user'] = userid2ids[row['user']]\n",
    "    row['artist'] = artistid2ids[row['artist']]\n",
    "    return row\n",
    "\n",
    "norm_df = df.apply(normalize, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can decide on a number of latent factors and split it out into training and validation sets. We also create a few variables that we will need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4969, 39217)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_factors = 40\n",
    "np.random.seed = 42\n",
    "msk = np.random.rand(len(norm_df)) < 0.8\n",
    "trn = norm_df[msk]\n",
    "val = norm_df[~msk]\n",
    "\n",
    "n_users = norm_df.user.nunique()\n",
    "n_artists = norm_df.artist.nunique()\n",
    "n_users, n_artists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the original example, we'll do a quick cross tab table of the top artists and most prolific users to sanity check how we are doing so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>artist</th>\n",
       "      <th>110</th>\n",
       "      <th>125</th>\n",
       "      <th>142</th>\n",
       "      <th>152</th>\n",
       "      <th>200</th>\n",
       "      <th>210</th>\n",
       "      <th>225</th>\n",
       "      <th>230</th>\n",
       "      <th>233</th>\n",
       "      <th>242</th>\n",
       "      <th>597</th>\n",
       "      <th>1029</th>\n",
       "      <th>1863</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.009803</td>\n",
       "      <td>0.012070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.047270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029743</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>0.001881</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>0.012480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026332</td>\n",
       "      <td>0.049207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.065194</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.087297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008724</td>\n",
       "      <td>0.090731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3689</th>\n",
       "      <td>0.026765</td>\n",
       "      <td>0.013753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005878</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019289</td>\n",
       "      <td>0.069680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024040</td>\n",
       "      <td>0.019981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "artist      110       125       142       152       200       210       225   \\\n",
       "user                                                                           \n",
       "588          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1295         NaN       NaN       NaN       NaN  0.005072       NaN       NaN   \n",
       "2186         NaN       NaN       NaN  0.009560       NaN       NaN       NaN   \n",
       "2350    0.001881       NaN       NaN  0.002796  0.012480       NaN  0.026332   \n",
       "2811         NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3499         NaN       NaN  0.012577       NaN       NaN       NaN       NaN   \n",
       "3689    0.026765  0.013753       NaN       NaN  0.005364       NaN       NaN   \n",
       "4002         NaN       NaN       NaN       NaN       NaN  0.000672       NaN   \n",
       "4247         NaN  0.016451       NaN       NaN       NaN       NaN       NaN   \n",
       "4671         NaN       NaN       NaN       NaN  0.008117       NaN  0.024040   \n",
       "\n",
       "artist      230       233       242       597       1029      1863  \n",
       "user                                                                \n",
       "588          NaN       NaN  0.003513  0.000567  0.009803  0.012070  \n",
       "1295         NaN       NaN       NaN  0.042796       NaN       NaN  \n",
       "2186    0.047270       NaN       NaN       NaN  0.029743       NaN  \n",
       "2350    0.049207       NaN       NaN       NaN  0.065194       NaN  \n",
       "2811    0.087297       NaN       NaN       NaN       NaN       NaN  \n",
       "3499         NaN  0.008724  0.090731       NaN       NaN       NaN  \n",
       "3689    0.005878       NaN  0.019289  0.069680       NaN  0.010615  \n",
       "4002    0.000532       NaN       NaN       NaN       NaN       NaN  \n",
       "4247    0.012010       NaN       NaN       NaN       NaN       NaN  \n",
       "4671    0.019981       NaN       NaN       NaN  0.009054       NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_artists = norm_df.groupby('artist')['plays'].count()\n",
    "top_artists = g_artists.sort_values(ascending=False)[:15]\n",
    "g_users = norm_df.groupby('user')['artist'].count()\n",
    "top_users = g_users.sort_values(ascending=False)[:15]\n",
    "\n",
    "top = norm_df.join(top_users, rsuffix='_r', how='inner', on='user')\n",
    "top = top.join(top_artists, rsuffix='_r', how='inner', on='artist')\n",
    "pd.crosstab(top.user, top.artist, top.plays, aggfunc = np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting table is a lot more sparse than the equivilent table for movie titles. I'm guessing this is because there is a much larger number of distinct artists than there are movies compared to the overall size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot product\n",
    "The most basic model as per the original example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_in = Input(shape=(1,), dtype='int64', name='user_in')\n",
    "u = Embedding(n_users, n_factors, input_length=1, W_regularizer=l2(1e-4))(user_in)\n",
    "artist_in = Input(shape=(1,), dtype='int64', name='artist_in')\n",
    "a = Embedding(n_artists, n_factors, input_length=1, W_regularizer=l2(1e-4))(artist_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = merge([u, a], mode='dot')\n",
    "x = Flatten()(x)\n",
    "model = Model([user_in, artist_in], x)\n",
    "model.compile(Adam(0.001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200214 samples, validate on 49997 samples\n",
      "Epoch 1/2\n",
      "200214/200214 [==============================] - 24s - loss: 0.0015 - val_loss: 9.0932e-04\n",
      "Epoch 2/2\n",
      "200214/200214 [==============================] - 24s - loss: 8.5566e-04 - val_loss: 9.0932e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f387c9db910>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.user, trn.artist], \n",
    "          trn.plays, \n",
    "          batch_size=64, \n",
    "          nb_epoch=2, \n",
    "          validation_data=([val.user, val.artist], val.plays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200214 samples, validate on 49997 samples\n",
      "Epoch 1/3\n",
      "200214/200214 [==============================] - 24s - loss: 8.5566e-04 - val_loss: 9.0932e-04\n",
      "Epoch 2/3\n",
      "200214/200214 [==============================] - 24s - loss: 8.5566e-04 - val_loss: 9.0932e-04\n",
      "Epoch 3/3\n",
      "200214/200214 [==============================] - 24s - loss: 8.5566e-04 - val_loss: 9.0932e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f387c9dbb10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit([trn.user, trn.artist], \n",
    "          trn.plays, \n",
    "          batch_size=64, \n",
    "          nb_epoch=3,\n",
    "          validation_data=([val.user, val.artist], val.plays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200214 samples, validate on 49997 samples\n",
      "Epoch 1/3\n",
      "200214/200214 [==============================] - 24s - loss: 8.5567e-04 - val_loss: 9.0934e-04\n",
      "Epoch 2/3\n",
      "200214/200214 [==============================] - 24s - loss: 8.5568e-04 - val_loss: 9.0933e-04\n",
      "Epoch 3/3\n",
      "200214/200214 [==============================] - 24s - loss: 8.5568e-04 - val_loss: 9.0933e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f382d0749d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit([trn.user, trn.artist], \n",
    "          trn.plays, \n",
    "          batch_size=64, \n",
    "          nb_epoch=3,\n",
    "          validation_data=([val.user, val.artist], val.plays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.5190e-12]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([np.array([588]), np.array([242])])\n",
    "# Should be roughly 0.003513"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias\n",
    "Something that we don't account for are users that listen to an extremely broad range of music. This would appear to give all of their artists lower ratings than other users. A user bias value might help this.\n",
    "I'm going to follow the course example and also assign a bias value to the artists as well.\n",
    "\n",
    "This is represented as a single value for each user and a single value for each artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, W_regularizer=l2(reg))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_in, u = embedding_input('user_in', n_users, n_factors, 1e-4)\n",
    "artist_in, a = embedding_input('artist_in', n_artists, n_factors, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bias(inp, n_in):\n",
    "    x = Embedding(n_in, 1, input_length=1)(inp)\n",
    "    return Flatten()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ub = create_bias(user_in, n_users)\n",
    "ab = create_bias(artist_in, n_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = merge([u, a], mode='dot')\n",
    "x = Flatten()(x)\n",
    "x = merge([x, ub], mode='sum')\n",
    "x = merge([x, ab], mode='sum')\n",
    "model = Model([user_in, artist_in], x)\n",
    "model.compile(Adam(0.001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200214 samples, validate on 49997 samples\n",
      "Epoch 1/1\n",
      "200214/200214 [==============================] - 26s - loss: 0.0016 - val_loss: 7.6381e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f382cf94050>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.user, trn.artist], \n",
    "          trn.plays, \n",
    "          batch_size=64, \n",
    "          nb_epoch=1, \n",
    "          validation_data=([val.user, val.artist], val.plays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200214 samples, validate on 49997 samples\n",
      "Epoch 1/3\n",
      "200214/200214 [==============================] - 26s - loss: 6.4063e-04 - val_loss: 7.7936e-04\n",
      "Epoch 2/3\n",
      "200214/200214 [==============================] - 25s - loss: 6.2755e-04 - val_loss: 7.9021e-04\n",
      "Epoch 3/3\n",
      "200214/200214 [==============================] - 26s - loss: 6.3193e-04 - val_loss: 7.9007e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f382bf14e90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit([trn.user, trn.artist], \n",
    "          trn.plays, \n",
    "          batch_size=64, \n",
    "          nb_epoch=3,\n",
    "          validation_data=([val.user, val.artist], val.plays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200214 samples, validate on 49997 samples\n",
      "Epoch 1/3\n",
      "200214/200214 [==============================] - 26s - loss: 6.2393e-04 - val_loss: 7.9219e-04\n",
      "Epoch 2/3\n",
      "185088/200214 [==========================>...] - ETA: 1s - loss: 6.2200e-04"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit([trn.user, trn.artist], \n",
    "          trn.plays, \n",
    "          batch_size=64, \n",
    "          nb_epoch=3,\n",
    "          validation_data=([val.user, val.artist], val.plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either these results are really good or something has gone wrong. Either way, we are overfitting. We shall save the weights and try a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path + 'bias.h5')\n",
    "model.load_weights(path + 'bias.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([np.array([588]), np.array([242])])\n",
    "# Should be roughly 0.003513"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
